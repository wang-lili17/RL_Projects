# Monte Carlo Reinforcement Learning - Blackjack

This project contains an implementation of Monte Carlo (MC) methods to solve the Blackjack environment using prediction and control strategies in reinforcement learning. It focuses on:

- Monte Carlo Prediction
- Monte Carlo Control
- Off-policy Prediction via Importance Sampling

These approaches estimate value functions and improve policies based on sampled episodes from the environment.

## Project Overview

Blackjack is a simplified, finite Markov Decision Process (MDP) that makes it ideal for studying model-free reinforcement learning techniques. This project uses first-visit Monte Carlo methods to evaluate policies and learn optimal strategies.

### Components

- **State**: Tuple of (player’s current sum, dealer’s showing card, usable ace flag)
- **Action**: Stick (0) or Hit (1)
- **Reward**: +1 for win, 0 for draw, -1 for loss

## Implemented Methods

### Monte Carlo Prediction

Estimates the value function \( V(s) \) for a given policy by averaging returns observed in sampled episodes.

### Monte Carlo Control

Learns an optimal policy by alternating between:
- Policy evaluation using MC prediction
- Policy improvement using an epsilon-soft greedy approach

### Off-policy Prediction via Importance Sampling

Estimates the value function for a target policy using episodes generated by a different behavior policy. Importance sampling ratios are used to adjust for the distribution shift.

## Visualization

The code includes visualizations of:
- State-value functions under different policies
- Optimal policies learned through control
- Convergence behavior (e.g., using MSE)

These are helpful for comparing the learned policies and understanding the performance of each method.

## Requirements

- Python 3.8+
- numpy
- matplotlib

Install dependencies with:

```bash
pip install -r requirements.txt
