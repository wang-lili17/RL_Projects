{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 10-armed Testbed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cada6500ddd403c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.bandit import Bandit\n",
    "\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def simulate(runs, times, bandits):\n",
    "    # region Summary\n",
    "    \"\"\"\n",
    "    For any learning method, we can measure its performance and behavior as it improves with experience over 1000 time steps \n",
    "    when applied to 1 of the bandit problems. This makes up 1 run. Repeating this for 2000 independent runs, each with a different \n",
    "    bandit problem, we obtained measures of the learning algorithm‚Äôs average behavior.\n",
    "    :param runs: Number of runs\n",
    "    :param times: Number of times\n",
    "    :param bandits: Bandit problems\n",
    "    :return: Optimal action count mean and reward mean\n",
    "    \"\"\"\n",
    "    # endregion Summary\n",
    "    \n",
    "    # region Body\n",
    "    \n",
    "    # Prepare a matrix filled with 0s for rewards\n",
    "    \n",
    "    \n",
    "    # Prepare a matrix filled with 0s for optimal action counts that has the same shape as rewards matrix\n",
    "    \n",
    "\n",
    "    # For every bandit\n",
    "    \n",
    "        # for every run\n",
    "        \n",
    "            # initialize bandit\n",
    "            \n",
    "            \n",
    "            # for every time step\n",
    "            \n",
    "                # select an action\n",
    "                \n",
    "                \n",
    "                # get the reward\n",
    "                \n",
    "                \n",
    "                # if the selected action is optimal for bandit\n",
    "                \n",
    "                    # change the corresponding 0 in the optimal action counts matrix to 1\n",
    "                    \n",
    "\n",
    "    return\n",
    "\n",
    "    # endregion Body"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be09fd89ebd40d84"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Reward Distribution"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4088366f60e51478"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot an example reward distribution\n",
    "plt.violinplot(dataset=np.random.randn(200, 10) + np.random.randn(10))\n",
    "plt.title(\"Figure 2.1\")\n",
    "plt.xlabel(\"Action\")\n",
    "plt.ylabel(\"Reward distribution\")\n",
    "plt.savefig(\"../generated_images/figure_2_1.png\")\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ed1daafa4064440"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Greedy Action Selection VS Œµ-greedy Action Selection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef67eb7574c5d2b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a list of epsilons with 0, 0.1 and 0.01 values\n",
    "\n",
    "\n",
    "# Create a list of bandits (1 bandit for every epsilon) where every bandit uses sample-average method\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a180bc790c31e65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define number of runs\n",
    "\n",
    "\n",
    "# Define number of times\n",
    "\n",
    "\n",
    "# Simulate optimal action counts and rewards\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "683805477a8d4606"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 20))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1a86ca5f4aefa2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "for epsilon, rewards in zip(epsilons, rewards):\n",
    "    plt.plot(rewards, label=\"$\\epsilon = %.02f$\" % epsilon)\n",
    "plt.title(\"Figure 2.2\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Average reward\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5536109f4e591e72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 2)\n",
    "for epsilon, counts in zip(epsilons, optimal_action_counts):\n",
    "    plt.plot(counts, label=\"$\\epsilon = %.02f$\" % epsilon)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"% Optimal action\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e6157d53f01223f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.savefig(\"../generated_images/figure_2_2.png\")\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca9dfed4b31f4579"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Optimistic Initial Values VS Realistic Initial Values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0c5945f58dd0dee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a list of 2 bandits where:\n",
    "# 1. 1st bandit: Œµ = 0, ùëÑ_1(ùëé) = 5, ùõº = 0.1,\n",
    "# 2. 2nd bandit: Œµ = 0.1, ùëÑ_1(ùëé) = 0, ùõº = 0.1\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "50d647979ced258a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define number of runs\n",
    "\n",
    "\n",
    "# Define number of times\n",
    "\n",
    "\n",
    "# Simulate optimal action counts\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3116e78a4c90c435"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1ae633f8632eed5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Upper-Confidence-Bound (UCB) Action Selection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7473708c239f1d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a list of 2 bandits where:\n",
    "# 1. 1st bandit: Œµ = 0, ùëê = 2, uses sample-average method,\n",
    "# 2. 2nd bandit: Œµ = 0.1, uses sample-average method\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1993531b4fe5feb2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define number of runs\n",
    "\n",
    "\n",
    "# Define number of times\n",
    "\n",
    "\n",
    "# Simulate average rewards\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e1fed28f6812c2e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d4db60f0153c024"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Gradient Bandit Algorithms (GBA)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5cb31b7d224bbba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a list of 4 bandits where:\n",
    "# 1. 1st bandit: uses GBA, ùõº = 0.1, uses average reward as baseline for GBA, expects true reward of 4,\n",
    "# 2. 2nd bandit: uses GBA, ùõº = 0.1, doesn't use average reward as baseline for GBA, expects true reward of 4,\n",
    "# 3. 3rd bandit: uses GBA, ùõº = 0.4, uses average reward as baseline for GBA, expects true reward of 4,\n",
    "# 4. 4th bandit: uses GBA, ùõº = 0.4, doesn't use average reward as baseline for GBA, expects true reward of 4\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1453e8fb0e6a32f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define number of runs\n",
    "\n",
    "\n",
    "# Define number of times\n",
    "\n",
    "\n",
    "# Simulate optimal action counts\\\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79a2acb7e523f0a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Labels\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67282242fae58cb9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2281e1a4dc8f1b9c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "974417449ca9770c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
